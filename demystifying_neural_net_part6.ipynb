{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Neural Network\n",
    "## Pytorch Tutorial 2\n",
    "\n",
    "- Learn about `torch.nn`, `torch.optim`, `Dataset`, `DataLoader` features of Pytorch \n",
    "\n",
    "### Intro\n",
    "- In the last Pytorch tutorial (part 3 of the series), I wrote about basic tensor operations. In this tutorial, I constructed a neural network to understand its architecture with MNIST digit dataset. I used this [excellent source](https://pytorch.org/tutorials/beginner/nn_tutorial.html) from Pytorch. This tutorial helped me the best in understanding neural network architecture as it first starts with only using basic tensor operations, but then gradually adding features such as  `torch.nn`, `torch.optim`, `Dataset`, `DataLoader`, so that one can see what each feature does. Since the source has excellent explanation, I tried to explain in Korean so that I am not merely just copying what is written in the tutorial. \n",
    "\n",
    "\n",
    "- Pytorch tutorial 1 에서는 간단한 tensor operation만 공부해 보았다. 이번 tutorial에서는 neural network 구조에 대해 공부하기 위해 직접 pytorch로 MNIST 숫자 데이터를 가지고 neural network를 만드는 연습을 해 보았다. Pytorch에서 제공하는 [tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html)을 보고 실습을 해 보았는데, pytorch 기능을 하나도 사용 하지 않고 시작하여, 하나씩 하나씩 pytorch기능들을 추가로 넣어줌으로서 pytorch feature들을 하나씩 알아볼 수 있는 아주 좋은 tutorial이니 꼭 확인 해 보시길!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "        content = requests.get(URL + FILENAME).content\n",
    "        (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH/FILENAME).as_posix(), \"rb\") as f:\n",
    "     ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- convert our loaded data to tensor object using `map` function\n",
    "- `map` 함수를 통해 로드 된 데이터를 텐서 object로 변환해 줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50000개의 hand written digit 이 있고, 28 x 28이미지는 flatten된 1d 텐서로 저장이 되어 있습니다 (1 row에 784)\n",
    "- there are 50000 hand written digit images and each image is 28 x 28. Images are stored as a flattened 1d tensor (784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), tensor(0), tensor(9))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target value has 10 classes 0 to 9\n",
    "x_train.shape, y_train.min(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net 만들기 Create a model only using Pytorch tensor operation\n",
    "- 기본적 pytorch tensor operation만을 사용하여 neural network를 만들어 보자. Weights와 bias를 만들기 위해 pytorch의 `torch.randn`을 사용하여 weights와 bias를 Xavier initialization으로 만들어 준다\n",
    "- Let's make a neural network only using basic tensor operations. First, we make weights and bias tensors by using `torch.randn` function. I will initialize weights with Xavier initalization.\n",
    "- Then, I'm going to use log softmax function to get prediction value, then use negative log likelihood as a loss function\n",
    "- log softmax 함수를 사용하여 예측값을 구하고, loss function으로는 softmax의 짝꿍인 negative log likelihood를 사용한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "weights = torch.randn(784, 10) / math.sqrt(784) # Xavier initialization multiplies 1/sqrt(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **after** initializing weights, we set `requires_grad_`. This function lets Pytorch to calculate the gradient during back propagation automatically.\n",
    "- `requires_grad_`을 weights 텐서 object를 만든 후, 아래와 같이 실행을 시켜 준다 `requires_grad_`는 자동적으로 Pytorch가 back propagation을 할 때, gradient를 계산하도록 하는 스위치를 켜주는 역할을 한다\n",
    "- `_` in Python signifies that the operation is performed in-place. There is also a different method to turn on `requires_grad` switch which is to give `requires_grad` parameter when creating tensor object. \n",
    "- Pytorch에서는 함수 뒤에 `_`가 붙으면 `inplace`역할을 합니다. bias에 대한 requires_grad를 실행해 줍니다. `requires_grad` 스위치를 켜줄 때 bias에서 만든 tensor처럼 스위치를 생성할때 안에 parameter로 줄 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I haven't seen `@` before I read this tutorial, but it stands for the dot product operation.\n",
    "- `@`는 pytorch tutorial을 읽으면서 처음 보았는데 벡터의 내적을 뜻한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1) \n",
    "# sum(-1) represents row sum and unsqueeze(-1) adds an additional dimension to the last index\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias) # @ represents dot product operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set batch size to 64. This represents one mini batch of 64 images. We call our function `model` on one batch of data. We just did one forward pass to get our predictions by passing in one batch of data. \n",
    "- 배치 사이즈를 64로 설정을 한다. 배치사이즈는 2의 거듭제곱으로 나아간다. GPU 성능에 따라 배치사이즈를 늘려갈 수 있다. 배치사이즈를 설정하고 위의 `model` function에 넣어 prediction을 구하면, 1 forward pass를 마친 것이다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5335, -1.8878, -2.5305, -2.1693, -1.8341, -2.2710, -2.9698, -2.4828,\n",
      "        -2.3980, -2.4458], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64 # batch size is normally power of 2 (64, 128, etc)\n",
    "xb = x_train[0:bs] # one mini batch of 64 images\n",
    "preds = model(xb)\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### negative log likelihood function\n",
    "\n",
    "- 아래 nll의 return 값을 보면 `-input[range(target.shape[0]), target].mean()`라고 되어있는데 여기서 `input`은 prediction값, `range(target.shape[0])`는 batch size인 64, `target`은 실제 target값이다. `input` 값이 log softmax로 prediction된 값 이므로 batch 64개의 `target` tensor object로 지정하여 index를 지정해 주면, 그 지정된 값으로 평균을 계산하여 negative log likelihood 값을 준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean() #target이 yb이고 각 index에 있는 것 mean\n",
    "loss_func = nll # set log likelihood as a loss function\n",
    "yb = y_train[0:bs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3172, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.Size([64, 10]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(preds, yb).dtype, preds.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### accuracy\n",
    "- for each prediction, if the index with the largest value matches the target value, then the prediction is correct. to caculate accuracy of our neural network, we get that \n",
    "- accuracy 계산은 `torch.argmax`를 통해 prediction값 중, 가장 큰 값의 index가 target값과 일치할 때의 값을 평균낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor([1,2,3,4,5,6,1,2])\n",
    "torch.argmax(a) # gives you the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0625)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb)) # 처음에 설정한 weight 값이 random이므로 첫 batch를 돌린 후 accuracy가 낮음을 확인할 수 있다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop. For each iteration :\n",
    "### Training loop 돌기 : \n",
    "- epoch별, batch별로 loop을 돌려야 하므로 각 이터레이션에서 해야하는 일은:\n",
    "    - batch size로 feed forward해줄 구간 설정\n",
    "    - feed forward로 예측값 내기\n",
    "    - loss 계산하기\n",
    "    - `loss.backward()`로 그레디언트를 계산하기 (`weight`과 `bias`의 gradient가 된다)\n",
    "    \n",
    "    - select mini-batch of data\n",
    "    - use the model to make predictions (feed forward)\n",
    "    - calculate the loss\n",
    "    - `loss.backward()` updates the gradients of the model, in this case, `weights` and `bias`\n",
    "    \n",
    "**`torch.no_grad()`** 안에서 weights과 bias의 gradient들을 update해야 한다. 그 이유는 아래의 부분이 gradient를 계산하는데 있어서 영향을 주면 안되기 때문이다. weights와 bias를 업데이트 하기 위함이지 gradient계산은 `loss.backward()`에서 끝났다. \n",
    "```\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n",
    "```\n",
    "그 후, 마지막 단계에서 gradient들이 쌓여 다음 배치에 영향을 주는 것을 방지 하기위해 **`grad.zero_()`** 를 사용하여 gradient를 0으로 재 설정 시켜준다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace #set_trace is useful for debugging in python\n",
    "lr = 0.5\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "#         set_trace()\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i: end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            # because we do not want below actions to be recorded for our next calculation of the gradient\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- without any hidden layer, we trained our first neural network. Let's compare the loss and accuracy after training and then those after only one batch of training. \n",
    "- hidden layer없이 첫 neural network를 train해 보았다. 전에 구했던 loss와 accuracy를 train이 끝난 후의 loss와 accuracy와 비교해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0812, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.functional 사용하여 neural network 만들기\n",
    "### using torch.nn.functional to build a neural network\n",
    "- 위에서는 basic tensor operation으로 neural network를 만들어 보았지만, 이번에는 위에서 직접 써주었던 softmax function과 negative log likelihood function을 `torch.nn.functional` 에 있는 함수들로 바꾸어 본다. 대부분 `torch.nn.functional` 를 `F`로 쓴다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위에서 사용한 log softmax activation 과 negative log likelihood는 pytorch에서는 `F.cross_entropy` 함수를 제공해 주어 두개의 함수를 하나의 function으로 사용이 가능하다. 그러기에 위의 log softmax 함수를 쓰지 않아도 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0812, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module과 nn.Parameter를 사용하기\n",
    "### Refactor using nn.Module and nn.Parameter\n",
    "- `nn.Module`과 `nn.Parameter`를 사용하면 training loop을 좀 더 깔끔하고 짧게 만들 수 있다. (여기서의 Module은 Python의 Module이 아님을 유의하자. `nn.Module`은 class이다)\n",
    "- `nn.Module`을 상속받아 우리에게 필요한 weights, bias, 그리고 feed forward function을 가지고 있는 class를 만드는 것이 neural network function을 관리하는데 있어서 가장 효율적인 방법이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module): # layer가 한개 밖에 없으므로 logistic regression과 같다\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10)/math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic() # neural network object를 생성해 준다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2093, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전 모델에서는 `weights`와 `bias`를 하나하나 업데이트 시켜 주었던 것과는 다르게, neural network를 object화 시킨다면, `model.parameters()` 라는 `model.zero_grad()` 는 알아서 모든 parameter들을 업데이트 시켜준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n-1)//bs+1):\n",
    "            start_i = i * bs\n",
    "            endi = start_i +1\n",
    "            xb = x_train[start_i:end_i]\n",
    "            yb = y_train[start_i:end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train 후, loss가 내려갔는지 확인 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0467, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Linear 사용하기\n",
    "### refactor using nn.Linear\n",
    "- 위에서 부터 계속 우리는 prediction값을 계산할 때 내적 operation을 사용하여 `xb @ weights + bias`를 사용해 왔다. Pytorch의 `nn.Linear`를 사용하여 직접 써 주었던 linear function을 간단하게 바꾸어 보자 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10) # input과 output을 parameter로 받는다\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2596, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0470, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim 사용하기\n",
    "### refactor using optim\n",
    "- Pytorch는 `torch.optim`으로 다양한 최적화 알고리즘을 제공한다. 깔끔하게 `step` method를 사용하면 optimizer가 1 step을 나아갈 수 있게 해준다. 그 말은, \n",
    "```\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "이 부분을\n",
    "```\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "``` \n",
    "로 한번에 처리를 할 수 있다는 것이다\n",
    "`optim.zero_grad()` 은 model.zero_grad()와 같이 gradient를 0으로 초기화 시켜주기에 다음 배치를 계산하기 전 실행시켜 주어야 하는 method 이다 \n",
    "\n",
    "model과 optimizer를 바로 만들어 낼 수 있는 `get_model` 함수를 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3319, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0827, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr = lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "print(loss_func(model(xb), yb))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "print(loss_func(model(xb),yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch의 Dataset class를 사용하여 쉽게 각 데이터에 접근하기\n",
    "### Refactor using dataset\n",
    "- pytorch의 dataset을 이용하면 각 데이터에 index값으로 쉽게 접근할 수 있다\n",
    "\n",
    "- 좀 전까지 \n",
    "``` \n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]\n",
    "```\n",
    "이 방식으로 데이터와 target 값에 각각 접근하여 training단계에서 mini batch를 가져왔다면,  \n",
    "`train_ds = TensorDataset(x_train, y_train)`로 train data와 target값을 묶은 후,  \n",
    "`xb,yb = train_ds[i*bs : i*bs+bs]` 로 한번에 mini batch를 가져올 수 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0117,\n",
       "         0.0703, 0.0703, 0.0703, 0.4922, 0.5312, 0.6836, 0.1016, 0.6484, 0.9961,\n",
       "         0.9648, 0.4961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1172, 0.1406, 0.3672, 0.6016,\n",
       "         0.6641, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.8789, 0.6719, 0.9883,\n",
       "         0.9453, 0.7617, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1914, 0.9297, 0.9883, 0.9883,\n",
       "         0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9883, 0.9805, 0.3633, 0.3203,\n",
       "         0.3203, 0.2188, 0.1523, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0703, 0.8555, 0.9883,\n",
       "         0.9883, 0.9883, 0.9883, 0.9883, 0.7734, 0.7109, 0.9648, 0.9414, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3125,\n",
       "         0.6094, 0.4180, 0.9883, 0.9883, 0.8008, 0.0430, 0.0000, 0.1680, 0.6016,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0547, 0.0039, 0.6016, 0.9883, 0.3516, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.5430, 0.9883, 0.7422, 0.0078, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0430, 0.7422, 0.9883, 0.2734,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1367, 0.9414,\n",
       "         0.8789, 0.6250, 0.4219, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.3164, 0.9375, 0.9883, 0.9883, 0.4648, 0.0977, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1758, 0.7266, 0.9883, 0.9883, 0.5859, 0.1055, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0625, 0.3633, 0.9844, 0.9883, 0.7305,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9727, 0.9883,\n",
       "         0.9727, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1797, 0.5078, 0.7148, 0.9883,\n",
       "         0.9883, 0.8086, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1523, 0.5781, 0.8945, 0.9883, 0.9883,\n",
       "         0.9883, 0.9766, 0.7109, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0938, 0.4453, 0.8633, 0.9883, 0.9883, 0.9883,\n",
       "         0.9883, 0.7852, 0.3047, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0898, 0.2578, 0.8320, 0.9883, 0.9883, 0.9883, 0.9883,\n",
       "         0.7734, 0.3164, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0703, 0.6680, 0.8555, 0.9883, 0.9883, 0.9883, 0.9883, 0.7617,\n",
       "         0.3125, 0.0352, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2148, 0.6719, 0.8828, 0.9883, 0.9883, 0.9883, 0.9883, 0.9531, 0.5195,\n",
       "         0.0430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.5312, 0.9883, 0.9883, 0.9883, 0.8281, 0.5273, 0.5156, 0.0625,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000]), tensor(5))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0] # tuple로 data와 target값이 들어가 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1104, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0818, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n-1)//bs+1):\n",
    "        xb, yb = train_ds[i * bs : i * bs + bs] # DataLoader를 써서 mini batch에 접근\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader를 사용하기 \n",
    "### Refactor using DataLoader\n",
    "- `DataLoader`는 배치를 다루는 역할을 가지고 있다. 위 처럼 데이터를 통해 만든 `Dataset` 으로 `DataLoader` 를 만들 수 있다.\n",
    "- `DataLoader`는 iterator 로서 training단계에서 batch를 iterate 할 때 유용하게 쓰인다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Dataset으로 만든 train_ds를 DataLoader의 argument로 넣어주고, batch size는 위에서 정한 64를 넣어준다 \n",
    "train_dl = DataLoader(train_ds, batch_size = bs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0815, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl: # dataloader를 iterate할 수 있다 \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set 추가하기 \n",
    "### Add validation set\n",
    "\n",
    "- 위에서는 training data로 loss를 확인하는 코드만 작성 했지만, 실제적으로는 **항상** validation set도 함께 loop안에 넣어, validation loss도 체크를 해 주어야 과최적화가 되고있는지에 대한 training상황을 확인할 수 있다\n",
    "- 맨 위에서 불러온 validation data로 `Dataset`과 `DataLoader`를 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size = bs*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 매 epoch마다 validation loss를 계산한 후, 출력해보자 \n",
    "- validation dataset이 생겼으니, `model.train()`은 training전 항상 켜주어야 하는 스위치 이고, `model.eval()`은 prediction을 하기 전 즉 loss 계산 전, 켜주어야 하는 스위치 임을 기억하자. 이유는, batch normalization `nn.BatchNorm2d` 을 해 주거나, dropout `nn.Dropout` 과 같은 과최적화를 막는 기능들이 training에는 필요한 요소들 이지만, validation에서는 꺼 주어야 하는 요소들이기 때문이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.2949)\n",
      "1 tensor(0.2972)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb,  yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "        \n",
    "    print(epoch, valid_loss / len(valid_dl)) #validation loss 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit()` 과  `get_data()` 만들기\n",
    "### create `fit()` and `get_data()`\n",
    "- `fit()` 과  `get_data()` 함수를 만들어 보자! 두개의 함수를 사용하면 위의 training과 validation loss 계산 단계를 3줄로 줄일 수 있다. \n",
    "- 먼저 `fit()` 안에서 사용할 `loss_batch` 라는 각 batch의 loss를 계산하는 함수를 만들어 준다. validation set에는 gradient를 계산할 필요가 없으므로, back propagation을 하지 않는다. 그러므로 함수의 parameter `opt`를 parameter로 설정하여, training할때는 켜주고, validation loss를 계산할 때는 꺼준다\n",
    "- `get_data()`는  Dataset을 받아 DataLoader를 리턴하는 함수이다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 batch의 loss를 계산\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_dl:\n",
    "            loss_batch(model, loss_func, xb, yb, opt) # training할 때는 optimizer 켜주기\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad(): # evaluation set 에는 gradient 계산 하지 않는다 \n",
    "            losses, nums = zip(*[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl])\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums) # loss의 average계산 \n",
    "        \n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs): \n",
    "    return (\n",
    "    DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "    DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단 3줄로 DataLoader 생성에서 부터, 모델을 만들고 training을 하기까지의 과정을 아래와 같이 나타낼 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.33802888598442077\n",
      "1 0.3175259089946747\n",
      "2 0.3022744032382965\n",
      "3 0.27204110164642337\n",
      "4 0.27117016434669494\n",
      "5 0.27682529377937315\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음 Pytorch tutorial에서는 CNN과 같은 특정한 architecture를 실습해 보려 한다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
