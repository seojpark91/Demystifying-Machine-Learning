{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Neural Network\n",
    "\n",
    "- Simple Perceptron\n",
    "- Perceptron, a decision maker\n",
    "- We need a Performance Function\n",
    "- Add a threshold to Perceptron, Make our Lives Easier\n",
    "- Here comes our Friend, Sigmoid Function\n",
    "- code implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "When I first heard about neural network, I won't lie. I was scared! The name 'neural network' also came from the fact that neural network behaves in a similar fashion to biological neurons.\n",
    "\n",
    "The word 'neural' itself seems awfully complicated (not that I almost failed Intro to Neuroscience class in college), yet it's combined with the word 'network'. Gosh. I am studying deep learning with awesome sources: *the Nature of Code* by Daniel Shiffman and *Neural Network with Deep Learning* by Michael Nielsen, and a MIT course on Artificial Intelligence taught by Professor Patrick Winston.\n",
    "\n",
    "In order to not get overwhelmed and frustrated by information overload, I decided to start writing short blog posts. These are intended to record my learning journey. By implementing codes in the books, summarizing and writing what I learned in my own language, I intend to grasp key concepts of deep learning. Without further ado, let's start by first talking about the basic structure of perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Perceptron\n",
    "In order to understand Neural Network, we first have to understand perceptron. Why? Because neural network is also called as a multilayer perceptron; a perceptron is a single neuron model, and a neural network is a \"network of perceptrons\" (often, 500,000 neurons). A perceptron takes one or more binary inputs and gives a simple binary output.\n",
    "\n",
    "![](img/blog1_figure1.png)\n",
    "![](img/blog1_figure2.png)\n",
    "\n",
    "A perceptron is a feed-forward model. Inputs get weighted as they flow in, and the weighted inputs are summed. Weights represent the importance of the respective inputs to the outputs. Then, the weighted sum goes through an activation function. This process of perceptron's receiving inputs and generating an output is called feed-forward. As the perceptron has a binary output, the activation tells the perceptron to \"fire\" or not. Below figure shows a simple perceptron's feed-forward process.\n",
    "\n",
    "![](img/blog1_figure3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron, a decision maker\n",
    "Here's a simple example. Let's say there is a weekday wine tasting event at one of your favorite restaurants. There are multiple factors that affect your decision. Let's consider 3 factors and call them $x_1$, $x_2$, $x_3$.\n",
    "\n",
    "1. Is the restaurant not far from your work?\n",
    "2. Is your significant other interested in going?\n",
    "3. Is wine/cheese selection impressive? (very important factor for me!)\n",
    "\n",
    "If the restaurant is far from your work, $x_1$ has a value of 0. If your significant other is excited to go with you, then $x_2$ is 1. If the restaurant provides excellent wine/cheese selection, then $x_3$ is 1. Then these inputs are multiplied by the corresponding weights. Who wants to drive for an hour to the restaurant after tiring day at work? So I will choose a weight $w_1$ = 4. I still can enjoy wine tasting without my boyfriend (well, it will be less fun, but I mean wine and cheese. Who needs more?) So, I will choose a weight $w_2$ = 2. For $x_3$, if the selection is impressive, I will not miss it for sure. A weight $w_3$ = 5.\n",
    "\n",
    "This perceptron can serve as a decision making process. Now if we add up all $x$ values with the respective $w$ values, I would know whether I should go to the wine tasting event or not. Inputs, weights, and bias have a cumulative influence on output. By varying weights and thresholds, we end up with different decisions calculated by our perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a Performance Function\n",
    "Neural network is a function approximator. There should be a way to compare the true value with the predicted value generated by perceptron. We need a performance function to measure how well a perceptron is doing. For now, how about this?\n",
    "$$Performance =\\Vert{predicted - true}\\Vert$$\n",
    "This measures magnitude of the distance between two values. However, this turns out to be mathematically inconvenient.\n",
    "\n",
    "Instead of just taking the absolute value of the difference, we square it.\n",
    "$$Performance =-\\frac{1}{2}(predicted - true)^{2}$$\n",
    "(Does this function look familiar to you? It should be! This is **Mean Squared Error** that we use as a lost function for linear regression problems). When we train a neural network, we adjust weights and thresholds so that the network gives us the right answer. Thus, we take partial derivatives of above performance function with respect to weights and biases $(\\dfrac{\\partial P}{\\partial w_1}$, $\\dfrac{\\partial f}{\\partial w_2}$, $\\dfrac{\\partial f}{\\partial b})$. This helps us to measure how much improvements we get by changing weights and biases a little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a threshold to Perceptron, Make our Lives Easier\n",
    "For a notational convenience, mathematicians came up with the so called \"bias trick\" of adding a bias term to a perceptron. Then, our weighted sum can be written as:\n",
    "$$ a = \\sum_{i=1}^3 w_i x_i + b = w^T x + b$$\n",
    "And our activation function can be written as:\n",
    "$$z = h(a) = h \\left( w^Tx + b \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here comes our friend sigmoid function\n",
    "\n",
    "![](img/blog1_figure4.png)\n",
    "\n",
    "In order to use gradient descent, we need a continuous function. However, perceptron only gives us a simple binary output; it's a step function. We cannot use gradient descent on discontinuous function. Its derivatives are equal to 0 everywhere (at 0, it's not differentiable). Thus, we implement a sigmoid function, which will convert this step function into a continuous function.\n",
    "$$h(a) = \\sigma(a) = \\frac{1}{1+e^{-a}}$$\n",
    "It looks very different from a perceptron model that we have seen up till now, but this sigmoid perceptron behaves very much alike to our plain perceptron. If $a$ is extremely big, then $e^{-a}$ is extremely negative, and $h(a)$ becomes 1. On the other hand, if $a$ is extremely negative, then $e^{-a}$ is extremely positive, and $h(a)$ goes to 0 asymptotically.\n",
    "\n",
    "Furthermore, sigmoid function gives a very clean derivative.\n",
    "$$\\frac{d\\sigma(a)}{da} = \\sigma(a)(1-\\sigma(a)) = \\sigma(a)\\sigma(-a)$$\n",
    "So clean and easy, right?\n",
    "\n",
    "With sigmoid activation function, we can take partial derivatives of performance function with respect to weights that we have. Here, greek letter eta $(\\eta)$ represents step size (also called learning rate) sets a step size we take when following gradients. This tells us how much change in improvement we get when weights move a little. This can be written as follows:\n",
    "\n",
    "$$\\Delta w = \\eta(\\dfrac{\\partial P}{\\partial w_1}i +\\dfrac{\\partial P}{\\partial w_2}j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is a code for a simple perceptron implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, math\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    trainer object stores the inputs and the correct answer\n",
    "    \"\"\"\n",
    "    def __init__(self, x1, x2, answer):\n",
    "        self.inputs = []\n",
    "        self.inputs.append(x1)\n",
    "        self.inputs.append(x2)\n",
    "        self.inputs.append(1) # bias input built into its array\n",
    "        self.answer = answer\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_num, lr):\n",
    "        \"\"\"\n",
    "        create perceptron\n",
    "        arguments:\n",
    "        input_num: the number of inputs (i.e, 2 inputs and 1 bias = 3)\n",
    "        \"\"\"\n",
    "        self.weights = [random.uniform(-1,1) for i in range(0,input_num)]\n",
    "        self.lr = lr\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        self.sum = 0\n",
    "        for i in range(0,len(inputs)-1):\n",
    "            self.sum += inputs[i] * self.weights[i]\n",
    "        return self.__activate()\n",
    "\n",
    "    def __activate(self):\n",
    "        \"\"\"\n",
    "        the activation function\n",
    "        \"\"\"\n",
    "        if self.sum >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def train(self, inputs, target):\n",
    "        \"\"\"\n",
    "        tune all the weights\n",
    "        \"\"\"\n",
    "        guess = self.feedforward(inputs)\n",
    "        error = target - guess\n",
    "\n",
    "        for i in range(0, len(self.weights)-1):\n",
    "            self.weights[i] += self.lr * error * inputs[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next up: Training Neural Network\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
